{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework2_CNN_House_1_and_3_hidden_layers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDr3dfILDYp1aVqtMQLbJX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChizuNwosu/RealTime_MachineLearning/blob/main/Homework_2_Problem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "DqKz2iyK7FpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "csv_file = '/media/Housing.csv'\n",
        "df = pd.read_csv (csv_file)   \n",
        "print (df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPWqm_oSy8jo",
        "outputId": "a7b74b2f-eee0-474a-ea95-e364302f0c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        price  area  bedrooms  ...  parking  prefarea furnishingstatus\n",
            "0    13300000  7420         4  ...        2       yes        furnished\n",
            "1    12250000  8960         4  ...        3        no        furnished\n",
            "2    12250000  9960         3  ...        2       yes   semi-furnished\n",
            "3    12215000  7500         4  ...        3       yes        furnished\n",
            "4    11410000  7420         4  ...        2        no        furnished\n",
            "..        ...   ...       ...  ...      ...       ...              ...\n",
            "540   1820000  3000         2  ...        2        no      unfurnished\n",
            "541   1767150  2400         3  ...        0        no   semi-furnished\n",
            "542   1750000  3620         2  ...        0        no      unfurnished\n",
            "543   1750000  2910         3  ...        0        no        furnished\n",
            "544   1750000  3850         3  ...        0        no      unfurnished\n",
            "\n",
            "[545 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting csv file to usable tensor data\n",
        "import csv\n",
        "priceS = []\n",
        "areaS = []\n",
        "bedroomsS = []\n",
        "bathroomsS = []\n",
        "storiesS = []\n",
        "parkingS = []\n",
        "\n",
        "price = []\n",
        "area = []\n",
        "bedrooms = []\n",
        "bathrooms = []\n",
        "stories = []\n",
        "parking = []\n",
        "\n",
        "with open(csv_file, 'r') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        priceS.append(row.get('price'))\n",
        "        areaS.append(row.get('area'))\n",
        "        bedroomsS.append(row.get('bedrooms'))\n",
        "        bathroomsS.append(row.get('bathrooms'))\n",
        "        storiesS.append(row.get('stories'))\n",
        "        parkingS.append(row.get('parking'))\n",
        "\n",
        "price = [int(numeric_string) for numeric_string in priceS]\n",
        "area = [int(numeric_string) for numeric_string in areaS]\n",
        "bedrooms = [int(numeric_string) for numeric_string in bathroomsS]\n",
        "bathrooms = [int(numeric_string) for numeric_string in bedroomsS]\n",
        "stories = [int(numeric_string) for numeric_string in storiesS]\n",
        "parking =  [int(numeric_string) for numeric_string in parkingS]\n",
        "\n",
        "t_pr = torch.tensor(price) # t_c\n",
        "t_ar = torch.tensor(area)   # t_u1\n",
        "t_be = torch.tensor(bedrooms)   # t_u2\n",
        "t_ba = torch.tensor(bathrooms)  # t_u3\n",
        "t_st = torch.tensor(stories)    # t_u4\n",
        "t_pa = torch.tensor(parking)    # t_u5\n",
        "\n",
        "# Normalization\n",
        "from math import sqrt\n",
        "def tensorToNormal(t_y):\n",
        "  t_y = t_y.float()\n",
        "  m = t_y.mean()\n",
        "  v = t_y.var()\n",
        "  s = sqrt(v)\n",
        "  norm = (t_y - m)/ s\n",
        "  return norm"
      ],
      "metadata": {
        "id": "HexuKNSw2Vam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_c = tensorToNormal(t_pr)\n",
        "t_u1 = tensorToNormal(t_ar)\n",
        "t_u2 = tensorToNormal(t_be)\n",
        "t_u3 = tensorToNormal(t_ba)\n",
        "t_u4 = tensorToNormal(t_st)\n",
        "t_u5 = tensorToNormal(t_pa)\n",
        "t_total = t_u1 + t_u2 + t_u3 + t_u4 + t_u5"
      ],
      "metadata": {
        "id": "gJue3bxn9gSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5OdTOlYBmjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c9208d7-3ae5-42a2-ab3b-8af4dda5289d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ]
        }
      ],
      "source": [
        "t_c = torch.tensor(t_c).unsqueeze(1) # <1>\n",
        "t_u1 = torch.tensor(t_u1).unsqueeze(1) # <1>\n",
        "t_u2 = torch.tensor(t_u2).unsqueeze(1) # <1>\n",
        "t_u3 = torch.tensor(t_u3).unsqueeze(1) # <1>\n",
        "t_u4 = torch.tensor(t_u4).unsqueeze(1) # <1>\n",
        "t_u5 = torch.tensor(t_u5).unsqueeze(1) # <1>\n",
        "t_total = torch.tensor(t_total).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Samples of five housing variables changes\n",
        "n_samples = t_total.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "\n",
        "train_indices, val_indices"
      ],
      "metadata": {
        "id": "P-y-FmDfDxw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0faef342-2e58-4865-fc77-5839563a365e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([193, 184,  97, 323, 312,  33, 239, 414, 503, 499, 197, 507, 437,\n",
              "         375,  43, 129, 172, 392, 328, 240, 357, 450,  78, 235, 434, 247,\n",
              "         513,  63, 317, 166, 429, 344,  61, 451, 110, 327, 386, 105, 497,\n",
              "         366, 222, 244, 406, 155, 270, 440, 346, 415,  24, 504, 182,  72,\n",
              "          45, 301, 395, 381, 153, 427,  64, 326, 385, 183, 461, 298, 151,\n",
              "          67, 533, 154, 486, 112,  58, 216, 520, 139, 501, 136, 211,   7,\n",
              "          14, 279, 463, 304, 416,  84, 532, 292, 149, 537, 113, 272, 147,\n",
              "         433, 425, 371, 514,  95, 305, 160, 106, 332, 267,  83, 372, 180,\n",
              "         118, 539, 308, 191, 148, 196, 468, 458, 133,   2, 283, 512, 176,\n",
              "         354, 505,  92, 207, 517, 322, 391, 189,  19, 493, 476, 320, 521,\n",
              "         349,  87, 475, 377, 237, 213, 156, 143,  75, 200, 203, 404, 396,\n",
              "         471,  94,  27, 412, 145,  15, 526, 157, 221, 438, 248, 127, 343,\n",
              "         177,  89, 494, 454, 275, 356, 382, 352, 484,  96, 421, 474, 490,\n",
              "         487,  49, 313, 194,  81,   5,  66, 347, 300, 181, 467,  34, 290,\n",
              "         466,  16, 173, 528, 265, 459, 448, 242,  25, 411, 422,   0, 410,\n",
              "         150, 163, 496, 228, 131, 428,   1, 245, 506, 470, 271, 274,  88,\n",
              "         223,  79, 126,  20, 121, 441, 426, 337, 444, 128, 280, 186, 266,\n",
              "         335, 462, 455, 130, 370,  69, 348, 407, 288, 361, 432, 345,   4,\n",
              "         260, 219, 212, 336, 523, 502, 388, 380, 397, 231,  44, 226, 360,\n",
              "          29, 393, 341,   9, 199, 276, 315, 307, 439, 485, 534, 289, 273,\n",
              "         405, 243, 350,  80, 329, 291, 167, 338, 282, 465, 383, 364, 367,\n",
              "         140, 469, 119, 286,  26, 362, 403,  40, 138, 123, 536, 252, 169,\n",
              "         342, 209,   3, 324, 134, 253, 330, 482, 420, 144, 489, 508, 208,\n",
              "         111, 510, 472, 495, 179,  36, 500, 185, 214, 538, 295, 408, 297,\n",
              "         424, 210, 205, 284, 530, 103,  23, 478, 401, 264, 100, 198,  46,\n",
              "         398, 165, 175,  77, 351,  17, 516, 492, 402, 187, 340,  10, 117,\n",
              "         321, 529, 498, 190, 218,  30, 519, 120,  18, 540,  98, 452, 400,\n",
              "         233, 436,  93,  31,  99, 479, 331,  74, 250,  38, 435, 353, 168,\n",
              "          32, 464, 373, 108, 170,   6,   8, 277, 453,  90, 229, 318,  51,\n",
              "         258, 325, 114,  12, 256, 378, 137, 281, 107, 319, 316,  73,  71,\n",
              "         195,  54, 299, 293,  65, 431,  68, 310, 217, 447, 161, 515, 309,\n",
              "         399, 115, 544,  52, 369, 524, 417, 294, 488, 220, 409, 206, 339,\n",
              "         522, 224, 102, 116, 365, 481, 456, 334, 413,  57, 443, 442,  21,\n",
              "         125,  86, 109, 254, 394, 132,  22]),\n",
              " tensor([ 82, 311, 446,  41, 142, 303, 376, 543, 287,  56, 278, 261,  42,\n",
              "         268, 542, 449, 241, 535, 527, 159, 460, 238, 333, 171, 204, 192,\n",
              "         232, 379, 234,  11, 227, 230, 384, 314,  91, 141, 162,  62, 262,\n",
              "         477, 509, 135, 483,  28, 249, 387,  53,  60,  50,  85, 146, 255,\n",
              "         202, 368, 257, 124,  59,  13, 101, 174, 236, 306, 445,  39,  55,\n",
              "         215, 358, 541, 390, 374, 251, 302, 296, 269, 389, 158, 480,  47,\n",
              "         188, 285, 473,  37, 263, 201, 178, 457, 419, 259, 418, 246, 355,\n",
              "         225,  76,  48, 491, 525, 164, 359, 122, 104, 518,  70, 152,  35,\n",
              "         511, 363, 423, 430, 531]))"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_u_train = t_total[train_indices]\n",
        "t_c_train = t_c[train_indices]\n",
        "\n",
        "t_u_val = t_total[val_indices]\n",
        "t_c_val = t_c[val_indices]\n",
        "\n",
        "t_un_train = 0.1 * t_u_train\n",
        "t_un_val = 0.1 * t_u_val"
      ],
      "metadata": {
        "id": "l64AoTVKD9zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val,\n",
        "                  t_c_train, t_c_val):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        t_p_train = model(t_u_train) # <1>\n",
        "        loss_train = loss_fn(t_p_train, t_c_train)\n",
        "\n",
        "        t_p_val = model(t_u_val) # <1>\n",
        "        loss_val = loss_fn(t_p_val, t_c_val)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss_train.backward() # <2>\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\"\n",
        "                  f\" Validation loss {loss_val.item():.4f}\")"
      ],
      "metadata": {
        "id": "YQEfA6L7jZ_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 1 part 1\n",
        "\n",
        "neuron_count = 8\n",
        "\n",
        "seq_model = nn.Sequential(\n",
        "    nn.Linear(1, neuron_count),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(neuron_count,1)\n",
        ")\n",
        "\n",
        "\n",
        "optimizer = optim.SGD(seq_model.parameters(), lr=1e-4)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 200, \n",
        "    optimizer = optimizer,\n",
        "    model = seq_model,\n",
        "    loss_fn = nn.MSELoss(),\n",
        "    t_u_train = t_un_train,\n",
        "    t_u_val = t_un_val, \n",
        "    t_c_train = t_c_train,\n",
        "    t_c_val = t_c_val) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CEY6GIUkMRf",
        "outputId": "7fe327dd-3b3a-411a-a6d1-34422593f7c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 1.0818, Validation loss 0.8917\n",
            "Epoch 10, Training loss 1.0817, Validation loss 0.8915\n",
            "Epoch 20, Training loss 1.0815, Validation loss 0.8913\n",
            "Epoch 30, Training loss 1.0813, Validation loss 0.8911\n",
            "Epoch 40, Training loss 1.0811, Validation loss 0.8909\n",
            "Epoch 50, Training loss 1.0809, Validation loss 0.8907\n",
            "Epoch 60, Training loss 1.0807, Validation loss 0.8905\n",
            "Epoch 70, Training loss 1.0805, Validation loss 0.8903\n",
            "Epoch 80, Training loss 1.0803, Validation loss 0.8901\n",
            "Epoch 90, Training loss 1.0801, Validation loss 0.8900\n",
            "Epoch 100, Training loss 1.0799, Validation loss 0.8898\n",
            "Epoch 110, Training loss 1.0797, Validation loss 0.8896\n",
            "Epoch 120, Training loss 1.0795, Validation loss 0.8894\n",
            "Epoch 130, Training loss 1.0793, Validation loss 0.8892\n",
            "Epoch 140, Training loss 1.0791, Validation loss 0.8890\n",
            "Epoch 150, Training loss 1.0789, Validation loss 0.8888\n",
            "Epoch 160, Training loss 1.0787, Validation loss 0.8886\n",
            "Epoch 170, Training loss 1.0785, Validation loss 0.8884\n",
            "Epoch 180, Training loss 1.0783, Validation loss 0.8882\n",
            "Epoch 190, Training loss 1.0781, Validation loss 0.8880\n",
            "Epoch 200, Training loss 1.0780, Validation loss 0.8878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 1 part 2\n",
        "\n",
        "neuron_count = 8\n",
        "\n",
        "seq_model = nn.Sequential(\n",
        "    nn.Linear(1, neuron_count),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(neuron_count, 13),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(13, 5),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(5,1)\n",
        ")\n",
        "\n",
        "\n",
        "optimizer = optim.SGD(seq_model.parameters(), lr=1e-4)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 200, \n",
        "    optimizer = optimizer,\n",
        "    model = seq_model,\n",
        "    loss_fn = nn.MSELoss(),\n",
        "    t_u_train = t_un_train,\n",
        "    t_u_val = t_un_val, \n",
        "    t_c_train = t_c_train,\n",
        "    t_c_val = t_c_val) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zOO0Cu4E9Pm",
        "outputId": "e2d8708b-b6fd-4a0c-a92d-23ce81bcfe36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss 1.1657, Validation loss 0.9894\n",
            "Epoch 10, Training loss 1.1650, Validation loss 0.9886\n",
            "Epoch 20, Training loss 1.1642, Validation loss 0.9878\n",
            "Epoch 30, Training loss 1.1634, Validation loss 0.9869\n",
            "Epoch 40, Training loss 1.1626, Validation loss 0.9861\n",
            "Epoch 50, Training loss 1.1618, Validation loss 0.9852\n",
            "Epoch 60, Training loss 1.1610, Validation loss 0.9844\n",
            "Epoch 70, Training loss 1.1603, Validation loss 0.9836\n",
            "Epoch 80, Training loss 1.1595, Validation loss 0.9827\n",
            "Epoch 90, Training loss 1.1588, Validation loss 0.9819\n",
            "Epoch 100, Training loss 1.1580, Validation loss 0.9811\n",
            "Epoch 110, Training loss 1.1573, Validation loss 0.9803\n",
            "Epoch 120, Training loss 1.1565, Validation loss 0.9795\n",
            "Epoch 130, Training loss 1.1558, Validation loss 0.9788\n",
            "Epoch 140, Training loss 1.1551, Validation loss 0.9780\n",
            "Epoch 150, Training loss 1.1544, Validation loss 0.9772\n",
            "Epoch 160, Training loss 1.1537, Validation loss 0.9765\n",
            "Epoch 170, Training loss 1.1530, Validation loss 0.9757\n",
            "Epoch 180, Training loss 1.1523, Validation loss 0.9749\n",
            "Epoch 190, Training loss 1.1516, Validation loss 0.9742\n",
            "Epoch 200, Training loss 1.1509, Validation loss 0.9735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ei6tlvK-YDAd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}